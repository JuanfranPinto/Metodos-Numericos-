{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7992775b",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center;\">ESCUELA POLITÉCNICA NACIONAL</h3>\n",
    "<h3 style=\"text-align:center;\">FACULTAD DE INGENIERÍA EN SISTEMAS</h3>\n",
    "<h3 style=\"text-align:center;\">METODOS NUMERICOS ICCD412 </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa4ccc",
   "metadata": {},
   "source": [
    "*JUAN FRANCISCO PINTO ANDRANGO*\n",
    "\n",
    "*Actividad Extracurricular 07b GPUs Hopper vs Blackwell*\n",
    "\n",
    "*GR1CC*\n",
    "\n",
    "*FECHA DE ENTREGA 28 DE ENERO DEL 2026*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc415b6c",
   "metadata": {},
   "source": [
    "#### Investigue sobre las diferencias entre las arquitecturas Hopper y Blackwell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99aebd",
   "metadata": {},
   "source": [
    "## Arquitectura Hopper \n",
    "\n",
    "#### Arquitectura NVIDIA Hopper \n",
    "\n",
    "Hopper escala de forma segura diversas cargas de trabajo en cada centro de datos desde pequeñas empresas hasta en la computacion de alto rendimiento. Construido con mas de 80 mil millones de transistores implementando el proceso TSM 4N, Hopper presenta innovaciones revolucionarias que impulsan las GPU *NVIDIA H200 Y H100* las que se conbiana para orfreser aceleraciones en entrenamineto e inferencia de IA generativa. \n",
    "- la arquitectura Hopper mejora la tecnologia Tensor Core con Trasformer Engine, que esta diseñado para acelerar el entrenamiento de IA \n",
    "- Los tensor cores aplicacion presiones combianadas de FP8 Y FP16 para acelerar significativamente los calculos de IA \n",
    "- Hopper Triplica las operaciones de punto flotante por segundo para las precisiones de TF32, FP64, FP16 e INT8\n",
    "- Los Cores de Hopper impulsan la aceleración de ornden de magnitud \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbdeff",
   "metadata": {},
   "source": [
    "## Arquitectura Blackwell\n",
    "\n",
    "#### Arquitectura NVIDIA Blackwell \n",
    "\n",
    "Blackwell aporta a la IA generativa y a la computacion acelerada. apartir de generaciones de tecgologias NVIDIA. Blackwell define un nuevo capitulo para la IA generativa con rendimiento y eficiencia. \n",
    "\n",
    "Blackwell esta formada por 208 mil millones de transistores, los que se fabrican con el proceso TSMC 4NP. cualquier producto Blackwell cuenta con dos matrices con reticula limitada conectadas con interconexion de chip a chip de 10 Terabytes por segundo (TB/s) en una sola GPU unificada.\n",
    "- Un transformer Engine utiliza tegnologia Blackwell Tensor Core para la aceleracion de inferencias y el entrenamiento para grandes modelos de lenguaje\n",
    "- Blackwell Tenso Cores agrega precisiones, como nuevos formatos de microescala para brindar alta precisión \n",
    "- Blackwell  utiliza tecnicas de escalado de microtensor para optimizar el rendimiento y precisión permitiendo\n",
    "- Duplica el rendimiento para tener una alta precisión\n",
    "- Blackwell incluye la Computación Confidencial de NVIDIA, para datos confidenciales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59fb17",
   "metadata": {},
   "source": [
    "### Diferencias entre las arquitecturas Hopper y Blackwell. \n",
    "\n",
    "1. Hopper es un chip tradicional con un limite fisico, Blackwell supera ese limite implementandon dos chips seperados que esta unidos por una conexion ultra rapida de (10TB/s)\n",
    "\n",
    "2. Presicion Numerica con Hopper (FP8) estandarizo el uso de presion de 8 bits para acelera los entrenamientos de IA, Blackwell maneja el soporte nativo para 4 bits con el Transformer Engine de segunda generacion \n",
    "\n",
    "3. Cuello de botella Hopper soporta 900GB/s de ancho de banda entre GPUs, Blackwell maneja 1.8TB/s con Bidireccional  que es una relevancia en la investigacion \n",
    "\n",
    "4. Motores Especializados Blackwell añade hardware específico para solucionar problemas que frenan a los investigadores y que Hopper manejaba por software o fuerza bruta\n",
    "\n",
    "Blackwell  ofrece estabilidad y escala masiva. En inferencia o investigación, el soporte de FP4 cambia las reglas del juego, permitiendo ejecutar modelos SOTA (State-of-the-Art) con una fracción de la energía y el coste que requería Hopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb66e99",
   "metadata": {},
   "source": [
    "### Preguntas de análisis\n",
    "\n",
    "¿Cuál es la diferencia entre FP32 vs TP32?\n",
    "\n",
    "\n",
    "FP32 maneja una presición simple mientras que TP32 se usa mas en inteligencia artificaial y es mas eficiente \n",
    "\n",
    "¿Qué representaciones de datos soportan estas GPUs (FP64, FP32, INT8)?\n",
    "\n",
    "- FP64 destaca en los calculos de ciencias exata como simulaciones y fisica aqui no puede existir fallos \n",
    "\n",
    "- FP32 Es un estandar antiguo para generar IA \n",
    "\n",
    "- INT8  se usa en IA para respuestas rapidas cuando una IA ya fue entrenada \n",
    "\n",
    "\n",
    "¿Por qué la nueva arquitectura prefiere representaciones numéricas con menor precisión?\n",
    "\n",
    "destaca el rendimiento por segundo cuando hay menor precisión podemos cargar mas operaciones\n",
    "el consumo de energia las operaciones de FP32 o FP64 consumen mucha energia, por eso al haber menor precisión existe mas eficiencias energetica \n",
    "Dar un mejor uso de memoria y ancho de banda con menor precisión como mas datos en caché, menos latencia y menos tráfico\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9622b23",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
